fitting ttraining data correctly but making poor predection is called "bias variance trade off" i.e you cant follow exactly where your training data goes
cross validation :
             for dividing data into train and test
             here we divide data into x parts and we pick x-1 for training and 1 for testing 
             normally ten fold classification is used i.e divide the data into 10 parts
confusion matrix:
                     actual results 
              t                              if sum along the diagonal is maxium for a model we will chosses that model the when prediction is actual resuts
              e
              s
              t
              r
              e
              s
              u
              l
              t
              s
 sensitivity : no of true prections/acutual value
 specificity : no of false predctions / actual false
 bias : the difference in true and predicted value 
 variance : the change in bias with different datasets 
 a good model have to have low bias and low variance 
 suprise : sigma(p(X)*log(1/p(X))
           -sigma(p(x)*log(p(x))
           
           
           
           least square fit line(gradient desent)
           
           
           //  import numpy as np 
import matplotlib.pyplot as plt 
def error(price,price_pre):
    cost=np.sum((price-price_pre)**2/len(price))
    return cost
def gradient_decent(x,y,it=1000,learrning_rate=0.001):
    learrning_rate=0.00001
    price=y
    slope=0.01
    intercept=0.01
    n=float(len(x))
    previous_cost=None
    for i in range(it):
        price_pre=slope*x+intercept
        cost = error(price,price_pre)
        if(previous_cost is not None and previous_cost-cost<=1e-6): break
        previous_cost=cost
        slope_derivative=-2/n*sum(x*(price-price_pre))
        intercept_derivative=-2/n*sum(price-price_pre)
        slope=slope-learrning_rate*(slope_derivative)
        intercept=intercept-learrning_rate*(intercept_derivative)
    print(f"slope:{slope},intercept{intercept}")//
    
    
    
    
    
    
    
    
    
